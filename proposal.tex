\documentclass[]{article}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{indentfirst}
%\usepackage[colorinlistoftodos]{todonotes}



%opening
\title{Unpaired Text-to-Image-to-Text Translation using Cycle Consistent Adversarial Networks}
\author{Jeremy Ma, Satya Krishna Gorti}
\date{}

\begin{document}

\maketitle


\section{Introduction}

Text-to-Image synthesis is a challenging problem that has a lot of room for improvement considering the current state-of-the-art results. Synthesized images from existing methods give a rough sketch of the described image but fail to capture the true essence of what the text describes. The recent success of Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} indicate that they are a good candidate for the choice of architecture to approach this problem.
\\

However, the very nature of this problem is such that a piece of text can map to multiple valid images. The lack of such a direct one-to-one mapping means that traditional conditional GANs \cite{mirza2014conditional} cannot be used directly. We draw our inspiration from the recent works of image-to-image translation \cite{liu2017unsupervised}\cite{zhu2017unpaired} where a cycle consistent GANs have been trained and achieved very impressive results.
\\

We believe that using a cycle GAN for text-to-image-to-text translation will generate better results than existing approaches and give more photo-realistic images. The added advantage of framing the problem in a cycle consistent manner would also mean that the architecture can not only be a text-to-image synthesizing network but also an image captioning network.
\\

Therefore, we have two generators $G$ and $F$. We train a mapping $G: T_{emb} \mapsto Y$ and inverse mapping $F: Y \mapsto T_{emb}$ in a cycle consistent manner, where $T_{emb}$ is an embedding for the text that describes an image. The generators $G$ and $F$ have their corresponding discriminator $D_g$ and $D_f$.

\section{Related work}

Generative Adversarial Networks (GANs) have achieved impressive results in problems such as image generation [site chintala]. Conditional GANs introduced in \cite{mirza2014conditional} build on top of GANs by learning to approximate the distribution of data by conditioning on an input.
\\

In the recent past there have been attempts on text to image synthesis using conditional GANs such as \cite{reed2016generative}\cite{dash2017tac}\cite{zhang2017stackgan}\cite{DBLP:journals/corr/abs-1710-10916}. We can see promising results in \cite{reed2016generative} by conditioning the GAN on text descriptions instead of class labels. \cite{zhang2017stackgan} uses a similar approach but breaks the process of generation down into two stage process. Stage-1 produces a low-resolution image based on text description. Stage-2 takes Stage-1 result as input and generates high resolution photo-realistic images. \cite{dash2017tac} additionally conditions its generative process with both text and class information and has produces superior results compared to [scott]. The embeddings to represent text used in the aforementioned papers is Skip-Thought vectors \cite{kiros2015skip}.
\\

Cycle consistent GANs have showed excellent results for multimodal learning problems, which lack a direct one-to-one correspondence with input and output and allows the network to learn many mappings at the same time as shown in \cite{zhu2017unpaired}\cite{liu2017unsupervised}\cite{zhou2016learning}. But to the best of our knowledge, it still hasn't been used for text-to-image generation, which is a similar multimodal learning problem.
\\
\section{Project plan}

\section{Nice-to-haves}

\bibliographystyle{plain}
\bibliography{ref}

\end{document}
